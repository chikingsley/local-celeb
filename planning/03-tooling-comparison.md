# Local-First Voice Assistant Pipeline: Comprehensive Tooling Comparison

Building a local-first application for voice cloning and assistants involves a multi-stage pipeline. The goal is to go from a movie file to an interactive voice agent that speaks with a cloned character's voice. This requires tools for obtaining transcripts, aligning and segmenting audio by character, enhancing voice quality, training text-to-speech (TTS) models, and deploying the result in an assistant with memory. Below we break down each pipeline component, comparing open-source tools (favoring local/offline use) and noting top-tier API-based alternatives as secondary options. We also highlight which tools have GUI, CLI, or library support for integration into a desktop app (like a MacWhisper-style GUI).

## Movie Script and Subtitle Sources

Accurate text transcripts are the foundation for alignment and subsequent steps. Sources for movie scripts or subtitles include:

- SimplyScripts, IMSDb, Subscene, etc.: Websites like SimplyScripts and IMSDb host movie screenplays (often original scripts with character cues). These can provide speaker-labelled dialogue if available. Subscene is a popular site for user-contributed subtitles (timed dialogue in SRT format). While these sites have no official APIs, developers often use community tools or scrapers to fetch data (e.g. the SubtitleDB API or scripts on GitHub). A Python tool "SBOX” can download subtitles from SubDB (a free subtitle database) via API, and others have built CLI downloaders for Subscene or OpenSubtitles.
- OpenSubtitles: A massive subtitle database with an API and downloadable corpus. OpenSubtitles 2018 includes over 2.6 billion subtitle sentences across 60 languages (over 1 million subtitle files in a 23 GB dataset per one dump). This is a rich source for English and multilingual subtitles. Using their API (with an API key) or offline corpus, one can retrieve aligned subtitles by movie name or hash. Many open-source scripts exist to match a given video file's hash to the correct subtitle (e.g. OpenSubtitles Python library or Node scripts).
- GitHub Datasets: Researchers have compiled movie subtitle datasets. For example, SubtitleCF provides ~19k English movie subtitles matched to the MovieLens dataset and Lison & Tiedemann's aligned subtitle corpus (used in Opus and ParaCrawl projects) covers dozens of languages. While these datasets are mainly for research, they illustrate the availability of bulk subtitle data that could be packaged with an app for offline use.

Local vs API: For a local-first app, embedding a large subtitle database is impractical, but the app could offer to load a local subtitle file or attempt an offline search in a bundled index. A simpler approach is to let users supply subtitle files or use the movie's closed captions if available on disk. If internet is acceptable as a secondary path, integrating the OpenSubtitles API or a web scraper to fetch subtitles by movie name can improve convenience (with user consent).

## Automatic Speech Recognition (ASR) Tools

Accuracy considerations: Official transcripts or vetted subtitles are ideal for alignment. Fan-made subtitles vary in quality; the app might need to run an ASR to double-check or fill gaps. In the absence of any provided text, the pipeline can fall back on automatic speech recognition (ASR) to generate a transcript (addressed next).

If no reliable subtitles are available, ASR will generate a transcript from the movie audio. Modern ASR models especially OpenAI's Whisper and its variants offer high accuracy for English and many languages, even in offline implementations:

- OpenAI Whisper (and Whisper.cpp): Whisper is a state-of-the-art ASR model, open-sourced by OpenAI, known for very high accuracy on diverse audio. Several projects enable running Whisper locally:
- Whisper.cpp: A C++ port of Whisper that runs entirely on CPU (with quantization support for speed). It's CLI-driven and lightweight enough for local GUI integration (e.g. MacWhisper uses Whisper.cpp under the hood). Whisper.cpp can transcribe medium-quality audio in real-time or faster on modern CPUs when using smaller models or 4-bit quantized large models.
- Faster-Whisper: An optimized inference engine (by HuggingFace/OpenAI contributors) using CTranslate2. It supports GPU and multi-threaded CPU inference for Whisper models, often achieving 2-4x faster than the reference PyTorch implementation. This is accessible via Python (pipeline or CLI) and can be used in a local app if Python is embedded or as a backend service.
- WhisperX: An extension of Whisper that adds forced alignment and speaker diarization. WhisperX first runs Whisper for transcription, then uses a VAD (Voice Activity Detection) and alignment model to produce word-level timestamps, and can integrate with speaker diarization models (like Pyannote) to tag speaker turns. It's a Python tool/CLI, suitable if you want precise word timings for alignment without a separate alignment step. GUI note: WhisperX is headless but could be run in the background; however it has more dependencies (PyTorch, etc.), so whisper.cpp or faster-whisper might be easier for a simple GUI app, using WhisperX only if word-level timing or diarization is critical.
- Whisper "v3" and fine-tuning: OpenAI hasn't formally released a new Whisper version as of 2025, but the community refers to various improved or fine-tuned models as “Whisper v2/v3." For example, the Transformers team fine-tuned Whisper Large for 5 hours to create a whisper-large-v2 checkpoint that slightly improves performance. Tools like Unsloth provide recipes to fine-tune Whisper or other STT models on custom data (using techniques like FlashAttention 2 and CPU offloading to speed up training). Fine-tuning Whisper on the specific movie's audio could theoretically improve accuracy for jargon or accents, but this requires transcripts of that audio which is exactly what we're trying to get. Thus in our pipeline, Whisper will likely be used zero-shot. Fine-tuning could be a phase-2 consideration if building a specialized model for a series or domain, using frameworks like Unsloth (which supports Whisper and other STT) to reduce training time.
- Alternatives (Wav2Vec2, Kaldi, APIs): Facebook's Wav2Vec2 models or Microsoft's QuartzNet can also be run locally via HuggingFace, but Whisper's accuracy on open-domain speech is generally higher. Traditional Kaldi recipes or Mozila DeepSpeech could be used but are now outperformed by transformer models. If we consider cloud APIs as a secondary track: Google Cloud Speech-to-Text or AssemblyAI or OpenAI's Whisper API can provide excellent results quickly, at a cost. For an offline app, these would only be fallback options when internet is enabled.

Comparison and Integration: Whisper (all variants) supports English and multilingual speech. The large model is about 1.5GB, but smaller models (Tiny, Base) can be shipped (the MacWhisper app includes medium and small models for example). For best accuracy on movie audio (which may have multiple speakers and background noise), Whisper Large (multilingual or English-only) is recommended. In a GUI app, one might include Whisper.cpp with large-v2 model (possibly quantized to 8-bit or 4-bit) to run on CPU. If GPU is available, faster-whisper or even direct PyTorch Whisper can be used for faster processing. Tools like DeepSpeed can optimize Whisper's inference on GPU (for instance, by using half-precision or batching) and were used in research to transcribe hour-long audio efficiently, but for a desktop app these optimizations may be less critical than portability.

In summary, Whisper remains the top ASR choice for local-first pipelines. Whisper.cpp offers maximum compatibility (C++ library/CLI usable from Python, Swift, etc.), while WhisperX adds useful features (alignment, diarization) at the cost of complexity. Fine-tuning is rarely needed for just transcript alignment purposes, but tools exist for experimentation.

## Forced Alignment Tools

Forced alignment takes a transcript and an audio file and produces precise timing for each word or phoneme. This is especially useful if we have an untimed script (like a screenplay) that we need to sync to the movie audio, or if we want to refine Whisper's timestamps. Key tools:

- Montreal Forced Aligner (MFA): A widely used open-source aligner built on Kaldi. MFA can align at word and phoneme level and supports dozens of languages (with pretrained acoustic models and pronunciation dictionaries for each). It achieves very precise alignment (on the order of ~10ms) by using Viterbi decoding with acoustic models trained on large speech corpora. Given our context (mostly English movies), MFA's pre-trained English model and dictionary would suffice. It outputs TextGrid files or CSV with start/end times for each word. Integration: MFA is a command-line tool (now installable via pip or Conda). It can be invoked by the app (pointing to the audio and transcript files), but note it can be memory/CPU intensive for long audios. One strategy is to use Whisper to segment the audio into smaller chunks (e.g., by silence or scene) and align each chunk's text with MFA for fine timestamps.
- Gentle (Kaldi-based aligner): An older but lightweight aligner for English that wraps Kaldi's alignment in a simpler package. Gentle outputs word times and even marks which words didn't align well. It's easier to run (just a single binary or Docker) and might be sufficient if MFA is too heavy. However, Gentle isn't as actively maintained as MFA.
- PolyglotDB (Speech Corpus Tools): PolyglotDB is a system for managing and querying corpora with phonetic alignment data. It actually uses MFA or other aligners under the hood to import data, then stores it in a database to allow complex queries (e.g., find all instances of a phoneme, analyze durations across speakers, etc.). While powerful for linguistic research, PolyglotDB likely exceeds our needs - it's essentially a backend database+GUI that might be used to analyze the aligned speech once we have it. Unless the user wants to explore the speech data, PolyglotDB isn't needed for just building the assistant. (It's also Python-based and not trivial to integrate into a standalone app without its environment.)
- Kaldi itself: For completeness, one could use Kaldi directly by writing a custom alignment script (given a trained model and lexicon). But this is exactly what MFA provides out-of-the-box. Unless we foresee needing a highly custom alignment (e.g., aligning languages or dialects unsupported by MFA), using MFA (or Gentle) is preferable to reinventing with raw Kaldi.
- WhisperX alignment: As mentioned, WhisperX internally performs a lightweight alignment by using a pretrained wav2vec-U model to force-align Whisper's own transcript. This yields word-level timestamps without an external dictionary, which is convenient. If we already plan to use WhisperX for diarization, we get alignment "for free" in that pipeline. However, MFA's alignment might still be more accurate if the transcript diverges from actual spoken words (MFA can handle slight paraphrases via optional word insertion/deletion penalties, etc.).

Comparison: For accuracy in aligning known transcripts, MFA is the gold standard - it was shown to produce alignments within ~10ms of human alignment on benchmark data. It supports multiple languages and custom acoustic model training if needed. The downside is that it requires a pronunciation dictionary (which it has for English, but for say Klingon or fictional words one might need to add entries) and computing resources. In contrast, WhisperX's alignment is quick and doesn't need a dictionary, but might not handle mis-matched transcripts as robustly.

In our pipeline, if we have a screenplay with speaker labels, we would likely use MFA to align each line to the audio. This would give precise timecodes for each character's lines. If using only a subtitle (which already has timing), forced alignment may be unnecessary unless we want to refine boundaries or obtain word-level granularity. In summary, MFA (local, CLI) is the top choice for forced alignment, with WhisperX as a faster inline alternative. Both have Python bindings or can be called via subprocess, fitting a GUI workflow (though alignment might be a background task given its lengthier runtime on a full movie).

## Audio Segmentation and Editing Tools

Once we have timestamps for dialogue (by subtitle or alignment), we need to segment the movie into clips per character. This involves extracting the audio (and possibly video) for each character's lines. Additionally, we may want to automatically edit or process these clips. Several tools and libraries can assist:

- Speaker Diarization (Who spoke when): If our transcript does not have speaker labels, we must apply speaker diarization to label segments by speaker. The leading open-source solution is Pyannote Audio (pretrained neural diarization pipeline) which can achieve high accuracy in separating speakers on clean audio. Pyannote's pretrained model can tag each timestamped speech segment with a speaker ID (e.g., Speaker A, B, etc.) that can be clustered to actual characters. In practice, combining Whisper for transcript + Pyannote for diarization is a common recipe for transcribing multi-speaker audio with speaker labels. WhisperX even integrates Pyannote for this purpose. The downside is Pyannote models are large and require a decent CPU/GPU. As an alternative, Silero VAD (Voice Activity Detector) can first split speech vs silence, and then a simpler clustering (e.g., using MFCC + k-means) can separate speakers, though with less accuracy. For our pipeline, if we already have a script with speaker names, we might not need diarization; otherwise, using an approach like Pyannote or VBx (another diarization algorithm) helps split the audio by speaker turns.
- Auto-Editor: Auto-Editor is a command-line tool that can automatically cut video/audio based on silence or other criteria. For example, it can remove all silent parts of a video automatically. It can also cut by motion or other "first pass" rules (e.g., remove sections with no movement). In our context, Auto-Editor could be used to chop the movie by non-speech sections, or to extract only the spoken parts. However, since we ultimately want segments per character, Auto-Editor alone isn't enough – we'd still need to identify which speaker is in each segment. Auto-Editor does have a Python API and a CLI, making it scriptable for integration.
- MoviePy: A Python library for video editing, useful for programmatically splitting, trimming, and concatenating video/audio clips. Once we know the timecodes for a character's lines, we can use MoviePy (or even FFmpeg directly) to cut those segments out of the full movie. MoviePy can operate purely in Python, which might be convenient if the rest of the pipeline is Python-based. For example, after alignment we could call VideoFileClip("movie.mp4").subclip(t1, t2) for each line and write out the character's audio. If we also want the video snippet (perhaps to show the user which scene it came from), MoviePy handles that too, albeit with some performance overhead compared to direct ffmpeg.
- Frame (open-source AI video editor): Frame is a new open-source video editing tool that integrates AI assistance. It provides a professional timeline interface and an AI “assistant” agent for editing tasks. Notably, Frame can auto-detect scenes, audio peaks, and even perform tasks like auto clipping based on criteria. It's cross-platform (web/desktop) and extensible. In our use case, Frame could (in theory) integrate an agent that, for example, automatically creates tracks for each character's dialogue, given some AI recognition. It also mentions face detection and smart organization, which could help if video is involved (e.g., tagging scenes by which character appears). While Frame is more focused on general creators' needs (jump cuts, enhancements), its open nature means we could possibly incorporate our pipeline into it. For now, it's a promising GUI if we wanted to give users a visual way to adjust segments. Frame has a web UI and likely some Node.js underpinnings (given the GitHub repository structure), but one would need to dive into its plugin system to use it specifically for our pipeline.
- Audapolis - Transcript-based Editor: Audapolis is an open-source editor that works like Descript (a text-based audio editor). It automatically transcribes audio and lets you edit audio by editing text. It's designed for spoken-word content (podcasts, interviews, etc.) and keeps everything local (no cloud). In Audapolis's GUI, you see the transcript and can cut or move sections as if editing a document, and the audio edits follow. For our needs, Audapolis could be very useful to manually refine segments or remove filler words/noise after initial segmentation. It even supports editing video and mixed media via the transcript interface. Integration: Audapolis itself is a standalone GUI (with Windows, Mac, Linux builds). We might not integrate its code into our app, but we can learn from it or even include it as a module. It's written in Python (uses PyTorch for transcription and PyQt for GUI). At minimum, we know it's feasible to implement Descript-like functionality locally, which might be a feature for phase 2 (allowing users to manually adjust the character clips by text).

Audapolis GUI showing transcript-based audio editing. The interface displays the transcribed text and allows editing operations on the text, which correspondingly cut or rearrange the audio. Each speaker can be labeled (as seen on the left), enabling easy segmentation by speaker. This local tool demonstrates how users can refine audio segments by simply editing the transcript, rather than dealing with waveforms.

- ElevenLabs/PlayHT editors: ElevenLabs and PlayHT are primarily known for TTS (more in next section), but they also provide web-based interfaces to manage and edit audio output. For example, ElevenLabs has a simple editor where you can enter text, preview the synthesized speech, adjust some parameters, and download audio clips. While not “segmentation” tools for original audio, they show the kind of user-friendly UI commercial products offer. PlayHT's studio similarly allows organizing synthesized clips into a timeline. These are cloud services (with APIs to generate speech), so they don't directly help in extracting the original voice, but they set a quality bar for user experience. We note them to consider that our GUI might eventually need features like waveform display, play/pause, re-ordering clips, etc., to be on par with commercial offerings.
- General editing and FFmpeg: Underlying all these, FFmpeg remains the powerhouse for actually encoding/decoding media. A local-first app will likely include FFmpeg (or use MoviePy which calls it) to handle reading the movie file and writing out clips or processed audio. Also, if precise frame cuts of video are needed, FFmpeg can ensure accuracy and re-encoding. Tools like Auto-Editor and MoviePy ultimately rely on FFmpeg for final rendering.

In summary, for automated segmentation: we will use a combination of diarization/alignment to get speaker timestamps, then use MoviePy or FFmpeg to split out each character's audio (and video if needed). Tools like Auto-Editor can assist by removing non-speech or dead air automatically, and we might incorporate their logic for an initial pass (to chop the movie into speech segments before diarization, for example). For manual refinement, an Audapolis-like approach could be integrated later so that a user can review each character's clips and adjust boundaries via text editing. Open-source AI editors like Frame are on the horizon, which could eventually allow a more fluid, GUI-driven workflow with AI assistance (imagine telling an agent "remove all pauses and background noises” or “give me only Alice's lines as clips," and it executes those edits).

## Audio Enhancement and Voice Isolation

After extracting a character's voice clips, we often have movie background music or noise mixed in. To train a high-quality voice model (or to use the voice in an assistant), we need the cleanest audio of that character speaking. This is where noise removal and voice isolation come in:

- ElevenLabs Voice Isolator: In 2024, ElevenLabs (known for TTS) released a Voice Isolator tool that uses AI to remove background sounds from speech. It's a cloud service (with a web UI and API) that takes an audio file and outputs "crystal-clear speech" by stripping out noise, music, etc. It's marketed for film and podcast post-production, exactly our scenario. For example, you could upload a clip of dialogue with rain and traffic noise, and it will return just the voice. This tool is likely using a sophisticated speech enhancement model. According to ElevenLabs, Voice Isolator can handle files up to 1 hour and is billed per minute of audio (cost in "characters" similar to their TTS pricing). Quality: Based on user feedback, it performs impressively well, leaving the voice intact and natural while removing noise. As a secondary track, one could leverage this API to batch-process all character clips for a very clean dataset - but that means sending audio to the cloud, which a local-first approach tries to avoid.
- Adobe Enhance Speech: Adobe's Enhance Speech (part of Adobe Podcast tools) is a free web-based AI filter that makes voice recordings sound “as if they were recorded in a professional studio". It removes background noise and echo and boosts clarity remarkably well. Many users have tried it on noisy audio and found the “improvement in audio quality was night and day" turning a muffled room recording into something like a close-mic studio voice. It's essentially noise removal + some EQ and upsampling magic. Enhance Speech is also an online tool (you upload audio on their site). Adobe has not released an offline version of this; however, their algorithms are likely based on research in speech enhancement and could be replicated with open models. It's notable that even older audio books or poor recordings have been revived by this tool though sometimes at the cost of slight artifacts or an "AI-cleaned" timbre.
- Open-Source Noise Reduction: There are several local solutions:
- RNNoise: A classic lightweight noise reduction library by Xiph.org (uses a small recurrent neural network). It's real-time capable and can be compiled into apps easily. It's good for constant background noise (like static or hum) but not as powerful for complex noise.
- DeepFilterNet: An open-source deep learning model for noise suppression (on GitHub) that can handle various noise types in speech. It requires 48 kHz audio input and produces clean speech output. This could be integrated as a local noise filter step.
- Facebook Demucs (speech mode): Demucs is known for music source separation. While originally for isolating vocals from music, its latest versions and variants (HDemucs etc.) can separate an audio into voice and noise components. There are models specifically fine-tuned to isolate speech from background. Running Demucs locally (it's a PyTorch model) could allow us to split each clip into "voice only" vs "background" and discard the background. It is heavy but effective for even overlapping speech and music.
- Resemblyzer/Resemble Enhance: We saw a GitHub project by Resemble AI for speech enhancement - this might be an open release of some of their proprietary tech. It could be explored for local use.
- CleanVoice and others: Some companies provide offline-capable SDKs for noise removal (e.g., Krisp.ai has an SDK for on-device noise cancellation). If targeting a specific platform (like macOS), one could also use platform APIs – for example, Apple's AVAudioEngine has a built-in mode for voice isolation (used in FaceTime voice isolation mode, available on macOS Monterey+). This is a system-level filter that could potentially be applied to audio streams with some work.
- Enhancement beyond noise removal: Apart from noise, voice enhancement might include normalization (equalizing volume across clips), de-reverb (removing echo if the movie scene had reverberation), and upsampling if needed. Open-source projects like VoiceFixer use AI to enhance old recordings (de-noise, de-reverb and even bandwidth extend). These could be considered if our character audio is very poor quality (say an old movie or a very noisy scene). Another aspect is isolating one speaker when multiple talk over each other - this is very hard, but if needed, one could attempt to separate speakers via source separation (not many tools do this well except research like speaker-separation models, e.g., “Conv-TasNet" type models could separate overlapping speech if the number of simultaneous speakers is small).

Workflow: For a local-first pipeline, a good approach is to integrate an open noise removal model that can run on the user's hardware. For example: 1. Use a VAD to chop out pure silence (Auto-Editor or Silero VAD). 2. For each character's audio clip, run a denoiser (RNNoise for fast/light, or DeepFilterNet for stronger noise suppression) to get a cleaner version. 3. Optionally, run an isolator/separation model if the background is music or heavy noise that simpler denoising can't handle – e.g. run Demucs to split voice vs music and take the voice track. 4. Monitor quality - sometimes aggressive filtering can distort the voice (artifacts). We might allow the user to toggle "enhance audio" on/off in case it changes the voice's character too much.

API as fallback: If high quality is paramount and the user opts in, we could send the clips to ElevenLabs Voice Isolator API for the best result. But this involves cost and uploading potentially copyrighted audio, so it must be optional. Similarly, Adobe's tool could be used manually by the user (since it's free, a user could choose to process the audio through their website outside our app).

In conclusion, top local tools for this stage are DeepFilterNet or Demucs for heavy-duty voice isolation, and RNNoise or PyTorch-based denoisers for lighter tasks. With these, we can get nearly studio-quality voice clips of each character. This clean audio is the training material for the next step: voice cloning.

## Voice Cloning and TTS Models

Now we have isolated voice audio per character. The next step is to train or fine-tune a TTS model so that our assistant can speak in that character's voice. We'll compare open-source TTS models for voice cloning versus proprietary services, focusing on those that can run locally or at least allow fine-tuning.

- Orpheus 3B (CanaryLabs): Orpheus is a 3 billion parameter Speech-Language Model released under Apache license in late 2024. It's essentially an LLM for TTS - built on a LLaMA architecture but outputs speech. Orpheus is notable for being open-source and high-quality, with an emphasis on expressive and emotive speech (it's described as “empathtic text-to-speech"). The 3B model can be quantized (there are GGUF files, e.g., 4-bit quantization to run on ~8GB VRAM). This model can be fine-tuned on a specific voice given training data. In fact, the authors provided a finetuned version ("Orpheus 0.1 FT") and even a Colab to run it. Because it's an autoregressive transformer, fine-tuning on a small dataset (like one movie's worth of lines) might require some caution (to avoid overfitting or it speaking only short sentences). Integration: Orpheus can be run in frameworks like LM Studio; it was noted one can load Orpheus in LM Studio and start a local TTS server. So, for a GUI, we might rely on an external runner (LM Studio or our own service) to host the Orpheus model and generate audio on demand. Orpheus's advantage is its licensing and quality - it's one of the first truly high-quality open TTS models that one could use commercially without fees.
- Sesame CSM-1B: Sesame Conversational Speech Model (CSM) 1B is another open model aimed at high-quality voice cloning. It's 1 billion parameters and designed to produce "lifelike AI speech with natural pauses and tone shifts". It's basically an ElevenLabs alternative that you can run locally. Sesame CSM supports voice cloning (one-shot/few-shot) and is geared towards generating conversational speech (with realistic hesitations, etc.). The model and a Gradio UI are available on Hugging Face and it can be run on consumer GPUs (with ~8GB+ VRAM). For integration, Sesame CSM has an OpenAI-compatible API mode (there are repos that let you query it similarly to how you'd query ElevenLabs via API). This makes it a strong candidate for drop-in use: once fine-tuned to a character, our app could send text to the local model and get audio, just like using a cloud TTS API but offline. Many in the community have praised CSM for its quality given its size, some calling it "a very good ElevenLabs free alternative".
- Coqui XTTS v2: Coqui (the makers of TTS library) released XTTS v2, a multilingual voice cloning model. It is capable of cloning a voice from just a 6-second sample audio and it can even transfer that voice to speak other languages (cross-lingual synthesis). XTTS v2 supports 17 languages out-of-the-box. This model powers Coqui's own TTS studio and API, but they made it available for local use (with a license that might restrict commercial use). The big appeal is zero-shot voice cloning: we might not even need to train if we have a clear voice sample from the character. We can feed a clip to XTTS and have it generate new lines in that voice. This is ideal for quick prototyping – for example, to get the assistant running without a lengthy fine-tuning. However, fine-tuning or providing more samples can improve quality for longer outputs. Coqui's TTS library can run this model and even fine-tune it (they provide docs for fine-tuning with their toolkit). Integration-wise, Coqui TTS is Python-based, but they also have an HTTP server mode. The model is large (not as big as Orpheus, perhaps a few hundred million parameters if it's an RNN or medium transformer), so running on CPU might not be real-time, but on a GPU it should be okay.
- MeloTTS (MyShell AI): MeloTTS is a high-quality multilingual TTS library with support for English, Spanish, French, Chinese, Japanese, Korean, etc. It's essentially an open-source engine for TTS that has gained a lot of attention (6k+ GitHub stars). It supports voice cloning with very little data. In fact, community projects built on MeloTTS, like OpenVoice2, claim you can "clone a voice with as little as 10 seconds of recording". This suggests MeloTTS uses a model architecture amenable to prompt-based cloning or extremely rapid fine-tuning (possibly similar to Microsoft's <10s voice clone demos). For our pipeline, MeloTTS could be a great backend to fine-tune on a character's lines. It likely uses a FastSpeech or VITS-like model under the hood (many modern TTS do two-stage: a spectrogram generator + vocoder). The advantage is speed and simplicity; training might be done in minutes, not hours, for a new voice since it might just embed the voice rather than full backprop on millions of weights. Integration: MeloTTS is Python-based. It can output audio given text and either a speaker embedding or after a quick training. The OpenVoice2 project indicates a one-click installer (Pinokio) to use MeloTTS for cloning, which implies it's accessible for end-users in some packaged form. This could be a candidate to embed in our app so a user can hit "Train voice” and it runs internally.
- StyleTTS / StyleTTS 2: StyleTTS is a research model (with a v2 and zero-shot variants) focused on achieving human-level naturalness by modeling speech "styles" via diffusion models. StyleTTS2 leverages diffusion and adversarial training with a speech-language model to produce extremely natural prosody. In theory, StyleTTS could yield the most human-like results, capturing nuances of the character's speaking style. However, as a research project, using it may require setting up the code and possibly a beefy GPU (diffusion models are slower to sample than traditional ones). It might not be ready for easy fine-tuning without significant ML expertise. We mention it because it represents the cutting edge that could close the gap to human speech, but for a practical v1 pipeline, one of the above turnkey models is preferable.
- FishSpeech: FishSpeech (by FishAudio) is another SOTA open TTS model, trained on over 1 million hours of speech (multi-lingual). It supports zero-shot and few-shot voice cloning – you can input a 10-30 second sample and it will generate speech in that voice. It doesn't rely on phoneme inputs and can handle any language script directly. FishSpeech prides itself on high accuracy and intelligibility (they report ~2% word error rate when transcribing its outputs). It is heavy, but they provide a user-friendly GUI (PyQt-based) and a WebUI for inference. They even have a "Fish Agent" which integrates ASR + LLM + TTS end-to-end for interactive chat, essentially a fully local voice assistant demo. This is very relevant: Fish Agent shows that it's possible to do the entire pipeline (speech in -> answer -> speech out) with local models in one system. We could draw inspiration or use parts of their pipeline for our assistant infrastructure. FishSpeech is open (Apache 2 for code, model weights CC BY-NC-SA for non-commercial). If licensing is acceptable (non-commercial for many voices), we could use it. Otherwise, Orpheus or Sesame (with Apache or similar licenses) would be better for commercial scenarios.
- ElevenLabs and PlayHT: On the proprietary side, ElevenLabs is currently considered the state-of-the-art in easy voice cloning. With just a minute of audio (and even less, unofficially), their model can mimic a voice with high fidelity and emotional range. It's an API/cloud only. PlayHT is a competitor offering a similar service (high-quality cloned voices via API, and some studio UI). For a local-first approach, we prefer open models, but in cases where a user might want the absolute best cloning with minimal effort, an integration to these APIs could be provided. E.g., the app could optionally upload the character's audio to ElevenLabs to create a custom voice profile there, then use their API to generate the assistant's speech. This would incur costs (ElevenLabs charges per character of output) and again raises privacy (uploading voice data). We treat this as a secondary fallback track for users willing to trade-off local for quality or convenience.

Comparison Summary: We have a rich selection of open TTS models. For local-first, the top picks are: Coqui XTTS v2 for instant cloning from a short sample, Sesame CSM-1B for high-quality conversational speech, FishSpeech for its all-in-one capabilities (though heavy), MeloTTS (OpenVoice2) for quick training on minimal data.

Any of these could do the job of producing the character's voice. It might even make sense to incorporate multiple: e.g., use Coqui XTTS for quick cross-language experiments (have the character speak other languages), and use Orpheus or Sesame for the final English voice if they produce more nuance.

Training vs Zero-shot: If we have plentiful audio of the character (a whole movie, possibly hours of speech), doing a fine-tune (full or LoRA) on a model like Orpheus or Sesame might yield the best result - the voice will be more consistent and handle longer sentences with the same tone. If we have limited data or need a quick turnaround, zero-shot models like XTTS or few-shot approaches like MeloTTS can produce a good clone without heavy training. Phase 1 could start with zero-shot cloning (no training, just feed one of the model a sample of the character), and phase 2 could attempt fine-tuning to improve quality.

Integration notes: Most open TTS models have Python APIs or Torch/HF integration. We will likely run the model in a Python backend and generate audio on the fly as needed. Real-time is not required (the assistant can have a short think/speak delay), but faster-than-real-time generation is possible for shorter texts on a decent GPU. If we want the voice generation to be part of a GUI app with minimal dependencies, one approach is using LM Studio or Ollama to serve the model:

- LM Studio supports running Orpheus and presumably can host TTS models with an OpenAI-like API.
- Ollama currently focuses on LLMs (text in/out), not TTS, but it could potentially be extended or used to run something like Orpheus if that model is packaged appropriately (Ollama deals with GGML models which Orpheus can be converted to). This might be more experimental.

Finally, whichever TTS we use, we must incorporate a vocoder if the model outputs spectrograms. Some models (like FishSpeech, Sesame) likely directly output waveforms (end-to-end). If not, we'd use a HiFiGAN or UnivNet vocoder model to synthesize the final audio from spectrogram.

## Voice Assistant Infrastructure

With a trained voice model, our final goal is an interactive assistant agent that uses the voice. This agent needs a few components: a language model (to generate the assistant's replies), a memory store (to maintain context between interactions), possibly tool-use abilities, and a serving framework to tie it together (voice input -> LLM -> voice output). We'll compare approaches:

- AI Town (by a16z Infra): AI Town is an open-source "virtual town" where multiple AI characters converse autonomously. It's essentially a multi-agent simulation with memory and personality. Under the hood, AI Town uses OpenAI GPT-4 (via API) for the brains, and Convex + Pinecone for memory/persistence. Each character has a persona and a memory log, so they remember past conversations and have individual goals. While our use-case is a single assistant (the user conversing with one character's persona), AI Town's architecture can be instructive. It provides:
  - A structure for agent personality prompts (each character had a description file with their identity and relationships).
  - A memory retrieval system: Pinecone vector DB to fetch relevant past dialogue as context.
  - A way to deploy this in a web/browser environment (though we might go for a desktop GUI).

AI Town is a full-stack project; to adapt it locally, one would replace the OpenAI API with a local LLM, and possibly replace Convex/Pinecone with local storage (like an embedded database or in-memory). In fact, the community has done this: there are forks of AI Town that use local models (e.g., integrating with Vicuna 13B via Ollama, etc.). A one-click installer called Pinokio can set up AI Town locally with Llama 2 models, enabling 100% local multi-agent chat. This demonstrates that AI Town's framework can support local-first operation given the right backend. We could use a simplified version of that for a single agent.

- Disler's Agent Stack: "Disler" refers to a developer active in the LLM community who has proposed lightweight agent frameworks. For instance, Just Prompt is a minimal orchestration that lets an LLM use tools when instructed. Disler's projects (like "single-file-agents" on GitHub) often show how to create an autonomous agent with minimal overhead. The idea is to avoid overly complex libraries and just use the LLM with a certain prompt pattern to manage tasks. For our assistant, a simplified approach might suffice since it mainly needs to maintain a conversation and possibly call some internal APIs (like storing memories or triggering actions). We might not need a heavy framework like LangChain; instead, we could implement a memory buffer ourselves and let the LLM generate responses. The mention of Disler indicates we should consider lean solutions: e.g., using a system prompt that includes character persona and some placeholder for memory, and have the model generate answers directly, rather than building a complex chain of thought.
- Convex Agent Stack: Convex is a cloud backend service (for data storage and serverless functions) which has been pushing an "agent stack" via their Stack blog. They have an Agent SDK that handles conversation history persistence and function calling, integrated with their database. Essentially, Convex can host the agent's state – for example, store all interactions and any long-term data the assistant should remember, and provide an API to query it. In a local context, using Convex (the hosted service) would break the local-first rule, but we could run a local equivalent (say, an SQLite or DuckDB for storing dialogue history, or just use the file system JSON). The key takeaway from Convex's approach is structuring the agent into layers: (LLM + memory + tools) where memory is externalized. We could mimic this by using a local vector store (like ChromaDB or FAISS) to keep an embedding of all past dialogues and retrieve relevant ones to prepend to the prompt (the classic ReAct or memory retrieval pattern). Convex's blog also emphasizes multi-agent scenarios and hosting, which might be more than we need for a single downloadable app. So we may not directly use Convex, but the concept of a persistent memory component remains important.
- Ollama: Ollama is a tool for running large language models on local machines via a simple CLI and API. It streamlines downloading models (especially finetuned ones like Llama-2 variants) and exposes a RESTful API (with an OpenAI-compatible interface) for querying those models. For our assistant, we could use Ollama to serve a local LLM (like a Vicuna or Mistral 7B) that generates the text responses. The advantage: we don't have to manage the model within our app's process; we delegate to Ollama, which can manage GPU/CPU usage and model loading. The disadvantage is requiring the user to install another component (though Ollama is relatively easy to bundle on Mac). Ollama's focus is purely the LLM; it doesn't handle memory beyond what you program into the prompts. But we can maintain memory in our app and send the model a prompt with conversation history each time. There are other similar projects (e.g., LMQL or LocalAI server), but Ollama is quite user-friendly and scriptable.
- LM Studio: We discussed LM Studio in the context of TTS and LLMs. LM Studio is a GUI that also can run as a local server with an OpenAI API compatibility layer. This means we could point our assistant application to LM Studio's server to use whichever model is loaded there. If a user already has LM Studio (which some AI enthusiasts might), they can use it to run, say, a Persona-Llama2 model fine-tuned for dialogues. LM Studio ensures no data leaves the machine, aligning with our goals. In a packaged app scenario, one could either instruct users to install LM Studio or possibly bundle a headless version of it. Another nice feature is LM Studio's ability to manage models and settings with a GUI - a power user could tweak the LLM parameters or try different models for the assistant without our app reinventing that wheel. However, relying on an external UI could be confusing, so perhaps this is more for advanced configuration.
- Agent Frameworks (LangChain, etc.): The user didn't explicitly mention LangChain, but it's a common solution for implementing an agent with memory, tools, etc. LangChain (Python/ TypeScript) could indeed be used to stitch together: the LLM, a VectorStore for memory, and custom tools (like calling the TTS or storing new info). Another emerging framework is AI Town's own toolkit (which might be Convex's agent SDK or others like Guidance by Microsoft or Haystack for QA). For Phase 1, a simpler custom implementation might suffice: e.g., maintain a list of last N user and AI utterances and prepend them to each prompt (context window permitting). For longer-term memory, when that list grows, use a vector store to fetch relevant pieces (there are lightweight vector DBs that run in-process, like Chroma which can be embedded).

Memory and Prompt Config: The assistant should have:

- A system prompt defining the character's persona (e.g., "You are Jack Sparrow, a witty pirate..." etc.).
- Possibly example dialogues to reinforce style.
- A mechanism to inject memory: e.g., search past convos for relevant facts if the user references something said earlier.
- Context management: ensure the prompt stays under the model's token limit by truncating or summarizing old interactions if needed.

This is what something like LangChain's ConversationBufferMemory or SummaryMemory would handle, but we can implement basic versions.

Voice integration: The assistant's pipeline will be: User speaks or types -> (ASR if voice input) -> text -> LLM -> response text -> TTS -> audio output. We have covered ASR and TTS. For voice input, we could use the same Whisper to transcribe user's microphone input in real-time or near real-time. That introduces additional latency but it's feasible. If focusing on a GUI, initial version might just be text chat + audio output, then add voice input.

Multilingual support: The question notes English is priority, but if tools allow, we can mention multilingual. With models like XTTS and our ASR supporting multiple languages, the assistant could possibly converse in non-English languages (some LLMs fine-tuned on multi-lingual data like Mistral or XLM-Vicuna would be needed for that). But that's an edge feature.

To summarize, for local assistant infra: a straightforward path is using a local LLM (7B-13B parameter range) that is reasonably good at conversation (perhaps a finetuned persona model), and managing a context history. Tools like AI Town show how to incorporate memory and multi-agent but can be simplified for one agent. We might not need the complexity of hosting a server if we embed a model directly (for example, running a GPT-2 or a distilled local model in-process might be simplest, though quality may suffer). Given the pace of development, by 2025 there are likely fine-tuned models specifically for roleplay/assistant in certain character styles, which we could leverage.

## Script Integration and Automation

Building this all into a downloadable GUI application (like MacWhisper or similar) presents some integration challenges and opportunities. We have multiple components (ASR, alignment, diarization, slicing, noise reduction, TTS, LLM, etc.) possibly written in different languages. To deliver a smooth user experience, we should plan how these pieces come together:

- Programming languages and GUIs: MacWhisper and SuperWhisper (Windows) are native GUI apps for Whisper. MacWhisper is a native Mac app (likely built in Swift or Objective-C) that calls the whisper.cpp library for transcription. Similarly, our app could be built in a native language (Swift for Mac, maybe a cross-platform framework for Windows/Linux) for the interface, while calling out to CLI tools or using libraries for the heavy tasks.
- Example: Use Swift for GUI, and use FFmpeg CLI for cutting video, Whisper.cpp C++ library for ASR, and perhaps wrap our TTS model as a C++ library too (some TTS like Coqui have C++ APIs, or we could use Python in the backend).
- The alternative is to build the whole app in Python (using PyQt or Kivy for GUI). Audapolis shows PyQt can be used to make a cross-platform UI. This might simplify calling Python-based tools (since everything runs in one Python process). We'd have to package it (PyInstaller or similar) which can be done.
- A third option is using a web-based GUI (Electron or a local web server & browser). Tools like Frame might be Electron. Electron allows calling Node.js, and from Node we can spawn Python processes or use Node libraries. For instance, Node has whisper.cpp bindings and even wasm builds for running in-browser (though heavy tasks likely need native). We could orchestrate tasks with JS and display a web UI. This can ease cross-platform distribution at the cost of higher memory.

- Automation/scripting tools: The mention of Hammerspoon suggests leveraging automation on macOS. Hammerspoon (with Lua scripts) can control applications and respond to system events. One could use it, for example, to watch a folder for new videos dropped in and automatically run the pipeline, or to bind a global hotkey to start/stop recording the mic for voice input. It could also help integrate with macOS-specific features (like controlling audio devices or using the Notification Center to show progress).
- However, embedding Hammerspoon in our app might be overkill; it's usually a separate utility. Instead, we could implement similar watchers or hotkeys in our app directly (using Swift or Python libraries).
- Hammerspoon is more useful if we want users to be able to extend or script our app's behavior. Perhaps in phase 2, advanced users could write Lua snippets to customize the assistant's triggers or interactions with other apps (e.g., have the assistant read the user's calendar from macOS - that would require automation permissions that Hammerspoon might manage).
- Apple Script / Shortcuts / Xcode automation: If targeting Mac specifically, we might integrate with Apple's Shortcuts (Automator's successor) to let the user create a shortcut like “Transcribe and clone voice from this movie" that they can run from Finder or via voice command. We can expose some of our app's functions via AppleScript or a command-line interface so that it can be scripted. For instance, provide a CLI movie2voice --in movie.mp4 --out voice.vocoder that runs the pipeline. Then users or automation can call it. Xcode's role here would be just to develop the app; "Xcode scripting" might refer to writing build scripts that package models or using Xcode's capability to run shell scripts (like bundling Whisper models into the app package).

- Bindings and CLI support: It's crucial that each component can be driven non-interactively. Here's a rundown:

- Whisper: available as CLI (whisper.cpp binary) and library (C++ API, also Python via transcribers like OpenAI's or faster-whisper).
- MFA: CLI tool (one can call mfa align via command).
- Pyannote: Python library – we'd likely call it in Python space, not a separate CLI.
- Auto-Editor: CLI (auto-editor video.mp4 --silent etc.) and also importable in Python.
- MoviePy: pure Python library.
- Frame: being a full app, doesn't have a simple library usage yet; we might not integrate it directly but possibly communicate if it had an API.
- Audapolis: also a full app. However, since it's open source, one could technically import parts of it or use its transcription/editing engine separately. But more realistically, we'd incorporate similar logic ourselves rather than depending on it.
- Noise reduction: RNNoise can be compiled as a C library and called from anything. Demucs/ DeepFilter are Python – might run them as subprocess or port to TorchScript/ONNX for integration.
- TTS models: Most we listed are Python/Torch. To integrate, we either run a Python process handling TTS or convert the model to a form usable by C++ (some can be converted to ONNX or there's efforts to bring TTS to faster runtimes - e.g., there are VITS models running in real-time on web now via WebGL).
- LLM: If we use Ollama, then sending a REST request is how we integrate. If we embed an LLM (like llama.cpp), there are C APIs for that as well. For example, llama.cpp offers a C interface which could be linked into a C++ or Swift app for directly querying a model. That's how some mobile apps run LLMs fully offline. This could eliminate the need for a separate server process.

Given the above, one approach:

- Write the core pipeline in Python (leveraging all the Python-friendly ML libraries).
- Then have a minimal GUI (could be a web-electron or a native wrapper) that calls that pipeline via a local HTTP server or CLI. For instance, the GUI could call pipeline.py --movie path --character "Alice" and the Python script does everything and yields a synthesized voice or a ready-to-chat agent (maybe the script could even launch a small chat window with text).
- This splits concerns but can introduce lag in communication. Alternatively, run Python in the same process as GUI using something like PyObjC or embedding Python in Swift (possible but can be complex).
- There's also Jupyter/IPython widgets approach for a quick GUI, but for a product-like app, not suitable.

Phase 2 considerations (which the question asks to outline next decisions/questions) will definitely involve these integration aspects:

- How to optimize for performance (do we need to quantize models more, use ONNX runtime, etc.).
- How to reduce model sizes for distribution (maybe offer downloads on first run to keep installer small).
- Cross-platform support (Windows may not run some of these as easily, what about Linux?).
- GUI/UX design (how technical vs simple, e.g., do we expose intermediate steps to users or keep it one-click?).
- Ethical/Legal: not asked here, but voice cloning raises IP issues - since asked to skip it, we won't discuss, but in real product that's a question to answer.

Now we will wrap up with a summary of top tools per category and recommendations for next steps.

## Summary of Top Tools and Next Steps

Top Tool Choices (Local-First vs API):

- Transcripts: Use local subtitle files or OpenSubtitles for offline retrieval. If no transcript, run OpenAI Whisper locally (via Whisper.cpp or Faster-Whisper) - the de facto ASR with high accuracy. As a backup, allow using OpenAI/Google STT APIs for possibly higher accuracy on tricky audio, if the user opts in.
- Alignment: Rely on Montreal Forced Aligner (offline, Kaldi-based) for precise word-level timing. If speed is a concern and Whisper's own timestamps suffice, skip MFA. (Later, explore WhisperX for a middle ground alignment with less overhead.)
- Segmentation & Diarization: For multi-speaker separation, use Pyannote (local diarization) integrated with Whisper output to label speaker turns. Then cut audio with MoviePy/FFmpeg programmatically. Employ Auto-Editor logic to remove silences and speed up processing. Provide a simple timeline view for users to review segments. (Secondary: if user just wants automatic splitting by silence and doesn't care who speaks, Auto-Editor can do it in one step.)
- Audio Enhancement: Integrate an open noise reduction model (e.g., DeepFilterNet or RNNoise) for each clip. For stronger isolation (voice vs background), use Demucs in speech mode. These keep data local. As a premium option, allow sending clips to ElevenLabs Voice Isolator API for maximum quality noise removal or instruct advanced users how to use Adobe Enhance externally for a final polish.
- Voice Cloning (TTS): For local: Coqui XTTS-v2 for instant zero-shot cloning (just from a short sample) and/or Sesame CSM-1B for high-quality speech generation with fine-tuning. Both can run on consumer GPUs. If the user has high-end hardware and wants the best open model, provide Orpheus 3B (perhaps as an optional download) - it's large but Apache-licensed and yields very natural speech. The TTS engine should be pluggable: we might let users choose or swap models. On the API side, ElevenLabs (via their API) is the gold standard; we'd include an option to use that for final voice if the user has an API key, as a benchmark or fallback. PlayHT or Azure Neural TTS could be alternatives in the cloud category.
- Assistant Brain (LLM & Memory): Locally, a fine-tuned Llama-2 13B or Mistral 7B model can serve as the conversation engine. Use a simple memory mechanism (e.g., keep last N turns + vector database for long-term facts). For ease, we can run this through Ollama (which can host models and expose an API). If the user has no suitable local model, allow connecting to an API (OpenAI GPT-4 or similar) as a secondary track – but clearly mark when data would leave the device. Possibly include a smaller model (like Llama-2 7B fine-tuned) bundled with the app for out-of-the-box functionality, and let power users swap in a larger model. For memory storage, embed a small ChromaDB to store conversation embeddings on disk, enabling the assistant to recall things said across sessions.
- Infrastructure & Integration: Use a modular design where each part can be invoked via CLI or API. For example, the app could have subcommands: transcribe, align, segment, train-voice, chat. This modular CLI helps testing each stage and also allows users to automate (e.g., script the whole pipeline or only certain parts). Provide a GUI that wraps these - possibly with tabs or steps for each stage (so users can see the transcript, adjust segments, then initiate voice training, then chat with the voice). Make sure to note tools that require extra downloads (some models might be large - allow managing them within the app, similar to how LM Studio lets you download models on first use).

Compatibility and GUI-friendliness: All chosen tools have either a permissive open-source license or offer a free tier for personal use. They have programmatic interfaces: Whisper.cpp, MFA, FFmpeg can run in background; Coqui TTS and others have Python libraries we can call. Our GUI will likely be custom-built, but we can take inspiration from Audapolis's approach to audio editing (text-based) and AI Town's approach to agent chatting. The GUI should allow the user to intervene if needed (e.g., correct a transcription if Whisper made an error that affects alignment, or listen to and fine-tune the cloned voice output if it's not quite right).

Next Decisions & Phase 2 Questions:

- Model Selection & Optimization: Which LLM for the assistant gives the best persona emulation? Should we fine-tune an LLM on the character's actual dialogue to better mimic their style of speech (not just voice)? This could be phase 2: e.g., fine-tune Llama-2 on the character's lines so it not only sounds like them (via TTS) but also uses their catchphrases or mannerisms in text generation.
- Performance Tuning: How to streamline the pipeline for speed? Can we do alignment and diarization in one pass (WhisperX already partially does this) to avoid re-processing audio? Should we use GPU for everything (requires managing memory so ASR, TTS, and LLM don't conflict)? Possibly implement a job queue to sequence tasks, and use smaller models where acceptable (e.g., Whisper medium instead of large, if real-time is needed).
- User Experience: How much control to give users at each step? E.g., do we expose the intermediate transcripts for editing (which could greatly improve final quality if corrected) or keep it fully automatic? Should the user be able to listen to the extracted voice clips and remove any with too much noise before training the TTS (maybe integrate a small audio player and waveform view)? Collecting feedback here will shape phase 2 – perhaps adding an “Advanced” mode to let users tweak and iterate on the voice model (for instance, removing segments where the character was whispering or shouting if that hurts the TTS training).
- Multilingual and Multi-character: Currently one character's voice is cloned from one movie. In phase 2, we might let the user select multiple characters (say they want a few different voices), or even provide voices from different sources. We should design our system to handle multiple voice profiles. Also, if the character speaks multiple languages in the movie (or we have subtitles in multiple languages), the TTS model choice might differ (some models handle multilingual synthesis - e.g., XTTS can make the character speak other languages with the same voice).
- Memory & Personalization: How to ensure the assistant retains knowledge from session to session? We might need to save conversation history to disk and reload it. Also, consider an interface for the user to inject knowledge into the assistant (for example, "assume the character is now in 2025 and knows about modern events" - that requires either fine-tuning the LLM or a mechanism to augment its knowledge base).
- Audio output quality: Evaluate if our chosen TTS yields satisfactory prosody and emotion. If the character's acting has a lot of emotion, can our model reproduce that? We might need to allow some prompting of the TTS model with style tokens or descriptors (some models accept input like "[angry] Hello there"). For ElevenLabs API, you can control intonation to a degree (stability and style sliders). For open models, perhaps we incorporate something like emotional context by adding an intermediate step: e.g., run an emotion classifier on the original line and use that to guide TTS (if the model supports it). This is experimental but worth exploring.

By addressing these questions, we can refine the tech stack in phase 2. The end vision is an app where a user can feed in media and, with minimal effort, spin up a personalized AI voice assistant that looks and sounds like their favorite character - all running locally on their machine, with full control over data and without reliance on cloud services unless explicitly desired.
